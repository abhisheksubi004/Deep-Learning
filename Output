Sigmoid Activation Function

After 1000 iterations, the total error is 0.8293347711039372
The final weight vectors are (starting from input to output layers)
[[-0.12357898 -0.19738837  1.15969899  0.40792582]
 [ 0.44878857 -0.57946959 -0.8412529   1.43408224]
 [ 0.55028296  0.40440947  1.17494448 -0.89300704]
 [ 1.05384906  1.74754507  0.04809154 -2.10089145]]
[[-1.57909226  0.40398454]
 [-0.27256798  1.01272439]
 [-0.2043283   1.00950448]
 [ 4.95505622 -3.85257292]]
[[-5.35377499]
 [ 5.49607638]]
Test Output : 0.1503340811453055


Tanh Activation Function

After 1000 iterations, the total error is 15.13574303880453
The final weight vectors are (starting from input to output layers)
[[  316.11262011   408.25403375 -4583.82237757   489.91064181]
 [ -418.53790411  -539.33862473   530.09632544  -557.162663  ]
 [  744.63165494  1026.55394064 -6713.01358882  1139.81893604]
 [  883.25865991  1219.19027094 -7419.97850617  1339.34420291]]
[[ 26.02384813  -5.84787061]
 [ 28.23158748  -5.66440897]
 [-86.81057476  -2.51866664]
 [ 30.95789005  30.36967184]]
[[ 4.49632223]
 [-5.4902674 ]]
Test Output : 25.28572584298362


ReLu Activation Function

After 1000 iterations, the total error is 30.75
The final weight vectors are (starting from input to output layers)
[[ 0.24672023 -0.96835751  0.85887447  0.38179384]
 [ 0.9946457  -0.65531898 -0.7257285   0.86519093]
 [ 0.39363632 -0.86799965  0.51092611  0.50775238]
 [ 0.84604907  0.42304952 -0.75145808 -0.96023973]]
[[-0.94757803 -0.94338702]
 [-0.50757786  0.7200559 ]
 [ 0.07766213  0.10564396]
 [ 0.68406178 -0.75165337]]
[[-0.44163264]
 [ 0.17151854]]
Test Output : 5.125
